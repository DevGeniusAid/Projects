{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e48ffbf9",
   "metadata": {},
   "source": [
    "## End to End Machine Learning Project\n",
    "An End to End machine learning model follows the complete lifecycle of a machine learning model which means to first collect data, then prepare the data according to the patterns found in the dataset, then train and evaluate the model and at last deploy the model. So an end to end machine learning project means to present a model in the form of an interactive application that runs the model on the dataset and produces output in the realtime according to the user input. In this article, I will take you through an end to end machine learning project using Python.\n",
    "\n",
    "An end to end machine learning project means to create an interactive application that runs our trained machine learning model and give output according to the user input. It follows the complete lifecycle of a machine learning model. Below are the steps that you need to follow while creating an end to end application for your model:\n",
    "\n",
    "1. Data access and collection\n",
    "2. Data preparation and exploration\n",
    "3. Model training and evaluation\n",
    "4. Model deployment\n",
    "\n",
    "Creating an end to end machine learning application is important to show most of your skills in a single project. In the section below, I will take you through how to create an end to end machine learning application using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee5f6f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 1\n",
      "Counter({'i': 1, 'love': 1, 'you': 1, '!': 1})\n",
      "Counter({'aly': 1, 'wins': 1, 'the': 1, 'gold': 1, '!!!': 1})\n",
      "Counter({'aly': 1, 'wins': 1, 'the': 1, 'gold': 1, 'aly wins': 1, 'wins the': 1, 'the gold': 1, '!!!!!': 1})\n",
      "Error: could not convert string to float: '\\\\rtf1' - Unable to convert label to float for item: \\rtf1\n",
      "features example: \n",
      "Counter({'rtf1': 1, '{\\\\': 1, '}': 1})\n",
      "Label example:\n",
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=1, test_size=0.8 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 75>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel example:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_all[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 75\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m123\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DictVectorizer\n\u001b[0;32m     79\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m DictVectorizer(sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2420\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2417\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2419\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2420\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2422\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2098\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2095\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2098\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2099\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2102\u001b[0m     )\n\u001b[0;32m   2104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=1, test_size=0.8 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "import re \n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import streamlit as st\n",
    "st.write(\"# Text Emotions Prediction\")\n",
    "t1 = st.text_input(\"Enter any text>>: \")\n",
    "\n",
    "def read_data(file):\n",
    "    data = []\n",
    "    with open(file, 'r')as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            label = ' '.join(line[1:line.find(\"]\")].strip().split())\n",
    "            text = line[line.find(\"]\")+1:].strip()\n",
    "            data.append([label, text])\n",
    "    return data\n",
    "\n",
    "file = 'text.txt'\n",
    "data = read_data(file)\n",
    "print(\"Number of instances: {}\".format(len(data)))\n",
    "\n",
    "def ngram(token, n): \n",
    "    output = []\n",
    "    for i in range(n-1, len(token)): \n",
    "        ngram = ' '.join(token[i-n+1:i+1])\n",
    "        output.append(ngram) \n",
    "    return output\n",
    "\n",
    "def create_feature(text, nrange=(1, 1)):\n",
    "    text_features = [] \n",
    "    text = text.lower() \n",
    "    text_alphanum = re.sub('[^a-z0-9#]', ' ', text)\n",
    "    for n in range(nrange[0], nrange[1]+1): \n",
    "        text_features += ngram(text_alphanum.split(), n)    \n",
    "    text_punc = re.sub('[a-z0-9]', ' ', text)\n",
    "    text_features += ngram(text_punc.split(), 1)\n",
    "    return Counter(text_features)\n",
    "\n",
    "print(create_feature(\"I love you!\"))\n",
    "print(create_feature(\" aly wins the gold!!!\"))\n",
    "print(create_feature(\" aly wins the gold!!!!!\", (1, 2)))\n",
    "\n",
    "def convert_label(item, name): \n",
    "    try:\n",
    "        items = list(map(float, item.split()))\n",
    "        label = \"\"\n",
    "        for idx in range(len(items)): \n",
    "            if items[idx] == 1: \n",
    "                label += name[idx] + \" \"\n",
    "        return label.strip()\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e} - Unable to convert label to float for item: {item}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "emotions = [\"joy\", 'fear', \"anger\", \"sadness\", \"disgust\", \"shame\", \"guilt\"]\n",
    "\n",
    "X_all = []\n",
    "y_all = []\n",
    "for label, text in data:\n",
    "    y_all.append(convert_label(label, emotions))\n",
    "    X_all.append(create_feature(text, nrange=(1, 4)))\n",
    "\n",
    "print(\"features example: \")\n",
    "print(X_all[0])\n",
    "print(\"Label example:\")\n",
    "print(y_all[0])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.2, random_state = 123)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vectorizer = DictVectorizer(sparse = True)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Classifiers \n",
    "svc = SVC()\n",
    "lsvc = LinearSVC(random_state=123)\n",
    "rforest = RandomForestClassifier(random_state=123)\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "clifs = [svc, lsvc, rforest, dtree]\n",
    "\n",
    "\n",
    "def train_test(clf, X_train, X_test, y_train, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_acc = accuracy_score(y_train, clf.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "    return train_acc, test_acc\n",
    "\n",
    "# train and test them \n",
    "print(\"| {:25} | {} | {} |\".format(\"Classifier\", \"Training Accuracy\", \"Test Accuracy\"))\n",
    "print(\"| {} | {} | {} |\".format(\"-\"*25, \"-\"*17, \"-\"*13))\n",
    "for clf in clifs: \n",
    "    clf_name = clf.__class__.__name__\n",
    "    train_acc, test_acc = train_test(clf, X_train, X_test, y_train, y_test)\n",
    "    print(\"| {:25} | {:17.7f} | {:13.7f} |\".format(clf_name, train_acc, test_acc))\n",
    "\n",
    "l = [\"joy\", 'fear', \"anger\", \"sadness\", \"disgust\", \"shame\", \"guilt\"]\n",
    "l.sort()\n",
    "label_freq = {}\n",
    "for label, _ in data: \n",
    "    label_freq[label] = label_freq.get(label, 0) + 1\n",
    "\n",
    "# print the labels and their counts in sorted order \n",
    "for l in sorted(label_freq, key=label_freq.get, reverse=True):\n",
    "    print(\"{:10}({})  {}\".format(convert_label(l, emotions), l, label_freq[l]))\n",
    "\n",
    "emoji_dict = {\"joy\":\"ðŸ˜‚\", \"fear\":\"ðŸ˜±\", \"anger\":\"ðŸ˜ \", \"sadness\":\"ðŸ˜¢\", \"disgust\":\"ðŸ˜’\", \"shame\":\"ðŸ˜³\", \"guilt\":\"ðŸ˜³\"}\n",
    "\n",
    "texts = [t1]\n",
    "for text in texts: \n",
    "    features = create_feature(text, nrange=(1, 4))\n",
    "    features = vectorizer.transform(features)\n",
    "    prediction = clf.predict(features)[0]\n",
    "    st.write(emoji_dict[prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b67792c",
   "metadata": {},
   "source": [
    "## End to End Machine Learning Project using Python\n",
    "I will be using the streamlit framework in python to create a web interface for interacting with the machine learning model. Below is the complete code to present this machine learning model in the form of an interactive web interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a1833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
